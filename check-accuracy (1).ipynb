{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":28903,"sourceType":"datasetVersion","datasetId":22535},{"sourceId":7047410,"sourceType":"datasetVersion","datasetId":4024170},{"sourceId":7047846,"sourceType":"datasetVersion","datasetId":4055311},{"sourceId":7155674,"sourceType":"datasetVersion","datasetId":4132358},{"sourceId":7182580,"sourceType":"datasetVersion","datasetId":4151748}],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install scipy==1.10.1 scikit-image==0.19.3 vit_keras==0.1.2","metadata":{"execution":{"iopub.status.busy":"2023-12-13T11:22:56.346586Z","iopub.execute_input":"2023-12-13T11:22:56.346844Z","iopub.status.idle":"2023-12-13T11:23:23.779415Z","shell.execute_reply.started":"2023-12-13T11:22:56.346819Z","shell.execute_reply":"2023-12-13T11:23:23.778483Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting scipy==1.10.1\n  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting scikit-image==0.19.3\n  Downloading scikit_image-0.19.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting vit_keras==0.1.2\n  Downloading vit_keras-0.1.2-py3-none-any.whl (24 kB)\nRequirement already satisfied: numpy<1.27.0,>=1.19.5 in /opt/conda/lib/python3.10/site-packages (from scipy==1.10.1) (1.24.3)\nRequirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.10/site-packages (from scikit-image==0.19.3) (3.1)\nRequirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-image==0.19.3) (10.1.0)\nRequirement already satisfied: imageio>=2.4.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image==0.19.3) (2.31.1)\nRequirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.10/site-packages (from scikit-image==0.19.3) (2023.8.12)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image==0.19.3) (1.4.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from scikit-image==0.19.3) (21.3)\nCollecting validators (from vit_keras==0.1.2)\n  Obtaining dependency information for validators from https://files.pythonhosted.org/packages/3a/0c/785d317eea99c3739821718f118c70537639aa43f96bfa1d83a71f68eaf6/validators-0.22.0-py3-none-any.whl.metadata\n  Downloading validators-0.22.0-py3-none-any.whl.metadata (4.7 kB)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->scikit-image==0.19.3) (3.0.9)\nDownloading validators-0.22.0-py3-none-any.whl (26 kB)\nInstalling collected packages: validators, scipy, vit_keras, scikit-image\n  Attempting uninstall: scipy\n    Found existing installation: SciPy 1.11.4\n    Uninstalling SciPy-1.11.4:\n      Successfully uninstalled SciPy-1.11.4\n  Attempting uninstall: scikit-image\n    Found existing installation: scikit-image 0.21.0\n    Uninstalling scikit-image-0.21.0:\n      Successfully uninstalled scikit-image-0.21.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\nkaggle-environments 1.14.3 requires scipy>=1.11.2, but you have scipy 1.10.1 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.10.1 which is incompatible.\ntensorflowjs 4.14.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed scikit-image-0.19.3 scipy-1.10.1 validators-0.22.0 vit_keras-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport cv2\nimport random\nimport shutil\nfrom tqdm import tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\nfrom PIL import Image\nfrom vit_keras import vit, utils","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-13T11:23:23.781620Z","iopub.execute_input":"2023-12-13T11:23:23.782408Z","iopub.status.idle":"2023-12-13T11:23:37.655717Z","shell.execute_reply.started":"2023-12-13T11:23:23.782369Z","shell.execute_reply":"2023-12-13T11:23:37.654736Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"<frozen importlib._bootstrap>:241: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n/opt/conda/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n\nTensorFlow Addons (TFA) has ended development and introduction of new features.\nTFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\nPlease modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n\nFor more information see: https://github.com/tensorflow/addons/issues/2807 \n\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nimport numpy as np\n\n\n#train_dir = '/kaggle/input/dogs-cats-images/dataset/training_set'\ntest_dir = '/kaggle/input/dogs-cats-images/dataset/test_set'\n\nclasses = os.listdir(test_dir)\n\nresize_size = 256\ncrop_size = 224\n\ndef preprocess_image(image):\n    image = tf.image.resize(image, [resize_size, resize_size], method=tf.image.ResizeMethod.BILINEAR)\n    image = tf.image.central_crop(image, central_fraction=crop_size / resize_size)\n    image = tf.math.divide(image, 255.0)\n    mean = [0.485, 0.456, 0.406]\n    std = [0.229, 0.224, 0.225]\n    image = (image - mean) / std\n\n    return image\n\ndef resize_and_crop(image, resize_size=256, crop_size=224):\n    # Resize with bilinear interpolation\n    resized_image = tf.image.resize(image, [resize_size, resize_size], method=tf.image.ResizeMethod.BILINEAR)\n    cropped_image = tf.image.central_crop(resized_image, central_fraction=crop_size / resize_size)\n    image = tf.math.divide(cropped_image, 255.0) #normalize\n\n    return image\n\ndef preprocess_for_attack(image):\n    mean = [0.485, 0.456, 0.406]\n    std = [0.229, 0.224, 0.225]\n    image = (image - mean) / std #다 normalize\n\n    return image","metadata":{"execution":{"iopub.status.busy":"2023-12-13T11:23:37.657052Z","iopub.execute_input":"2023-12-13T11:23:37.657795Z","iopub.status.idle":"2023-12-13T11:23:37.670949Z","shell.execute_reply.started":"2023-12-13T11:23:37.657756Z","shell.execute_reply":"2023-12-13T11:23:37.670122Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#xai 지표별 테스트 결과\ndef check_accuracy(test_dir, model_name):\n    tf_list = []\n    model = keras.models.load_model('/kaggle/input/dog-and-cat-classifier/'+model_name+\"_best_model.h5\", compile=False)\n    model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n    for data_dir in [test_dir]:\n        folderPath = data_dir\n        for i in tqdm(range(len(df))):\n            j = \"-\".join(df['path'].iloc[i].split(\"/\")[4:])\n            img = cv2.imread(os.path.join(folderPath, j))  # Read image\n            img = preprocess_image(img)\n            img_batch = np.expand_dims(img, axis=0)  # Expand dimensions\n            y_pred = model.predict(img_batch, verbose=0)  # Use img_batch here\n            y_pred_single_label = np.argmax(y_pred, axis=1)\n            tf_list.append(classes[int(y_pred_single_label)]==j.replace(\"-\", \".\").split(\".\")[-4])\n            #print(classes[int(y_pred_single_label)], j.replace(\"-\", \".\").split(\".\")[-4])\n    print(f\"model name: {model_name}, visualization: {test_dir.split('/')[-1]}\")\n    counts = Counter(tf_list)\n    for element, count in counts.items():\n        print(element, \":\", count)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\nimport pandas as pd\nimport sys\nimport io\n\ndf = pd.read_csv('/kaggle/input/dog-cat-pandas/cat_dog_df.csv', index_col=0)\ndf = df[df['divide']=='test']\nclasses = ['dogs', 'cats']\n\nfor model_name in ['vit', 'vgg']:\n    for vis in ['ifgsm_full', 'ifgsm_negative', 'ifgsm_super_pixel', 'ifgsm_pos+neg']:\n        #sys.stdout = io.StringIO()\n        test_dir = f'/kaggle/input/ifgsm-samples/IFGSM_Sample/{model_name}/test/' + vis\n        check_accuracy(test_dir, model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport cv2\nimport keras\nfrom keras.models import load_model\nfrom keras.optimizers import SGD\nfrom keras.callbacks import LearningRateScheduler, ModelCheckpoint\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\nfrom tensorflow.keras.applications import VGG16, EfficientNetV2L, ResNet50\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.models import Model\n\n#fine_tuning\nmodel_name = 'vit'\ndf = pd.read_csv('/kaggle/input/dog-cat-pandas/cat_dog_df.csv', index_col=0)\ntrain_df, val_df = train_test_split(df[df['divide']=='validation'], test_size=0.2, random_state=42)\ntest_df = df[df['divide']=='test']\nclasses = ['dogs', 'cats']\n\ndef learning_rate_schedule(epoch, start_epoch=10, end_epoch=30, start_lr=0.005, end_lr=0.0002):\n    if epoch <=5:\n        return (0.001 * epoch)\n    else:\n        decay_rate = math.pow((end_lr/start_lr), (1/(end_epoch-start_epoch)))\n        return (start_lr * math.pow(decay_rate, epoch - start_epoch))\n\ndef prepare_data(df, split):\n    X, y = [], []\n    folderPath = f\"/kaggle/input/ifgsm-samples/IFGSM_Sample/{model_name}/{split}/ifgsm_full\"\n    for i in range(len(df)):\n        path = df.iloc[i]['path']\n        j = \"-\".join(path.split(\"/\")[4:])\n        img = cv2.imread(os.path.join(folderPath, j))\n        img = preprocess_image(img)\n        X.append(img)\n        y.append(path.split(\"/\")[-2])\n    X = np.array(X)\n    y = np.array(y)\n    y = tf.keras.utils.to_categorical([classes.index(label) for label in y])\n    return X, y\n\ntrain_X, train_y = prepare_data(train_df, 'validation')\nval_X, val_y = prepare_data(val_df, 'validation')\ntest_X, test_y = prepare_data(test_df, 'test')\n\nbase_model_vit = vit.vit_b16(\n    image_size=224,\n    classes=2,\n    pretrained=True,\n    pretrained_top=False)\nnum_layers_to_unfreeze = int(0.3 * len(base_model_vit.layers))\nfor layer in base_model_vit.layers[:-num_layers_to_unfreeze]:\n    layer.trainable = False\nfor layer in base_model_vit.layers[-num_layers_to_unfreeze:]:\n    layer.trainable = True\nx = Flatten()(base_model_vit.output)\noutput = Dense(2, activation='softmax')(x)\nmodel = Model(inputs=base_model_vit.input, outputs=output)\nmodel = keras.models.load_model('/kaggle/input/dog-and-cat-classifier/'+model_name+\"_best_model.h5\", compile=False)\nmodel.compile(optimizer=SGD(), loss='binary_crossentropy', metrics=['accuracy'])\n\nlr_scheduler = LearningRateScheduler(learning_rate_schedule)\ncheckpoint_filepath = 'weights_{epoch:02d}.h5'\nmodel_checkpoint_callback = ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=False,\n    save_freq='epoch',\n    save_format='h5',\n    verbose=1\n)\n\nmodel.fit(train_X, train_y, validation_data=(val_X, val_y), epochs=10, callbacks=[lr_scheduler, model_checkpoint_callback])","metadata":{"execution":{"iopub.status.busy":"2023-12-13T11:36:45.483250Z","iopub.execute_input":"2023-12-13T11:36:45.484110Z","iopub.status.idle":"2023-12-13T11:39:46.938554Z","shell.execute_reply.started":"2023-12-13T11:36:45.484062Z","shell.execute_reply":"2023-12-13T11:39:46.937585Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/vit_keras/utils.py:81: UserWarning: Resizing position embeddings from 24, 24 to 14, 14\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n25/25 [==============================] - ETA: 0s - loss: 0.2479 - accuracy: 0.9388\nEpoch 1: saving model to weights_01.h5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n","output_type":"stream"},{"name":"stdout","text":"25/25 [==============================] - 28s 812ms/step - loss: 0.2479 - accuracy: 0.9388 - val_loss: 0.2405 - val_accuracy: 0.9350 - lr: 0.0000e+00\nEpoch 2/10\n25/25 [==============================] - ETA: 0s - loss: 0.2450 - accuracy: 0.9275\nEpoch 2: saving model to weights_02.h5\n25/25 [==============================] - 15s 593ms/step - loss: 0.2450 - accuracy: 0.9275 - val_loss: 0.2392 - val_accuracy: 0.9350 - lr: 0.0010\nEpoch 3/10\n25/25 [==============================] - ETA: 0s - loss: 0.2362 - accuracy: 0.9312\nEpoch 8: saving model to weights_08.h5\n25/25 [==============================] - 15s 592ms/step - loss: 0.2362 - accuracy: 0.9312 - val_loss: 0.2287 - val_accuracy: 0.9350 - lr: 0.0024\nEpoch 9/10\n25/25 [==============================] - ETA: 0s - loss: 0.2316 - accuracy: 0.9325\nEpoch 9: saving model to weights_09.h5\n25/25 [==============================] - 15s 592ms/step - loss: 0.2316 - accuracy: 0.9325 - val_loss: 0.2274 - val_accuracy: 0.9350 - lr: 0.0021\nEpoch 10/10\n25/25 [==============================] - ETA: 0s - loss: 0.2342 - accuracy: 0.9362\nEpoch 10: saving model to weights_10.h5\n25/25 [==============================] - 15s 590ms/step - loss: 0.2342 - accuracy: 0.9362 - val_loss: 0.2266 - val_accuracy: 0.9350 - lr: 0.0019\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.History at 0x7d57781d7d90>"},"metadata":{}}]},{"cell_type":"code","source":"##model testing\nfor epochs in range(1,11):\n    model = keras.models.load_model(f'weights_{epochs:02d}.h5', compile=False)\n    model.compile(optimizer=SGD(), loss='binary_crossentropy', metrics=['accuracy'])\n    tf_list = []\n    folderPath = f\"/kaggle/input/ifgsm-samples/IFGSM_Sample/{model_name}/test/ifgsm_full\"\n\n    for i in tqdm(range(len(test_df))):\n        j = \"-\".join(test_df['path'].iloc[i].split(\"/\")[4:])\n        img = cv2.imread(os.path.join(folderPath, j))  # Read image\n        img = preprocess_image(img)\n        img_batch = np.expand_dims(img, axis=0)  # Expand dimensions\n        y_pred = model.predict(img_batch, verbose=0)  # Use img_batch here\n        y_pred_single_label = np.argmax(y_pred, axis=1)\n        tf_list.append(classes[int(y_pred_single_label)]==j.replace(\"-\", \".\").split(\".\")[-4])\n        #print(classes[int(y_pred_single_label)], j.replace(\"-\", \".\").split(\".\")[-4])\n    print(str(epochs), \": adversarial samples\")\n    counts = Counter(tf_list)\n    for element, count in counts.items():\n        print(element, \":\", count)\n    tf_list = []\n    folderPath = f\"/kaggle/input/ifgsm-samples/IFGSM_Sample/{model_name}/test/original\"\n    for i in tqdm(range(len(test_df))):\n        j = \"-\".join(test_df['path'].iloc[i].split(\"/\")[4:])\n        img = cv2.imread(os.path.join(folderPath, j))  # Read image\n        #print(os.path.join(folderPath, j))\n        img = preprocess_image(img)\n        img_batch = np.expand_dims(img, axis=0)  # Expand dimensionsz\n        y_pred = model.predict(img_batch, verbose=0)  # Use img_batch here\n        y_pred_single_label = np.argmax(y_pred, axis=1)\n        tf_list.append(classes[int(y_pred_single_label)]==j.replace(\"-\", \".\").split(\".\")[-4])\n        #print(j.replace(\"-\", \".\").split(\".\")[-4], classes[int(y_pred_single_label)])\n    print(str(epochs), \": clean dataset\")\n    counts = Counter(tf_list)\n    for element, count in counts.items():\n        print(element, \":\", count)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T11:36:41.468369Z","iopub.status.idle":"2023-12-13T11:36:41.468767Z","shell.execute_reply.started":"2023-12-13T11:36:41.468565Z","shell.execute_reply":"2023-12-13T11:36:41.468583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n##model testing\ntf_list = []\nfolderPath = f\"/kaggle/input/ifgsm-samples/IFGSM_Sample/{model_name}/test/original\"\nfor i in tqdm(range(len(test_df))):\n    j = \"-\".join(test_df['path'].iloc[i].split(\"/\")[4:])\n    img = cv2.imread(os.path.join(folderPath, j))  # Read image\n    #print(os.path.join(folderPath, j))\n    img = preprocess_image(img)\n    img_batch = np.expand_dims(img, axis=0)  # Expand dimensionsz\n    y_pred = model.predict(img_batch, verbose=0)  # Use img_batch here\n    y_pred_single_label = np.argmax(y_pred, axis=1)\n    tf_list.append(classes[int(y_pred_single_label)]==j.replace(\"-\", \".\").split(\".\")[-4])\n    #print(j.replace(\"-\", \".\").split(\".\")[-4], classes[int(y_pred_single_label)])\nprint(f\"model name: {model_name}, visualization: {folderPath.split('/')[-1]}\")\ncounts = Counter(tf_list)\nfor element, count in counts.items():\n    print(element, \":\", count)\n'''","metadata":{"execution":{"iopub.status.busy":"2023-12-13T11:35:51.283802Z","iopub.status.idle":"2023-12-13T11:35:51.284132Z","shell.execute_reply.started":"2023-12-13T11:35:51.283956Z","shell.execute_reply":"2023-12-13T11:35:51.283970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\nepochs = list(range(1, 21))  # 예시로 1부터 20까지의 에폭을 사용\nlr_values = [learning_rate_schedule(epoch) for epoch in epochs]\n\nplt.plot(epochs, lr_values, marker='o')\nplt.title('Learning Rate Schedule')\nplt.xlabel('Epoch')\nplt.ylabel('Learning Rate')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"def ifgsm_attack(model, image, label, epsilon=0.001, num_iter=30, clip_min=0.0, clip_max=1.0):\n    adv_image = tf.identity(image)\n\n    for _ in range(num_iter):\n        with tf.GradientTape() as tape:\n            tape.watch(adv_image)\n            prediction = model(preprocess_for_attack(adv_image))\n            loss = tf.keras.losses.sparse_categorical_crossentropy(label, prediction)\n\n        gradient = tape.gradient(loss, adv_image)\n        perturbation = epsilon * tf.sign(gradient)\n\n        adv_image = tf.clip_by_value(adv_image + perturbation, clip_min, clip_max)\n\n    #adv_image_np = adv_image.numpy().squeeze()  # Squeeze to remove channel dimension\n    del gradient, perturbation\n\n    return adv_image\n","metadata":{}},{"cell_type":"markdown","source":"import sys\nimport io\nimport gc\nimport ctypes\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nimport cv2\nimport random\nimport shutil\nfrom tqdm import tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.python.client import device_lib\nimport tensorflow.keras as keras\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\nfrom PIL import Image\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\nfrom vit_keras import vit, utils\nimport lime\nimport skimage\nimport shap\nimport pandas as pd\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\nfrom lime.lime_image import LimeImageExplainer\nimport ctypes\n\n\n\ndef preprocess_image(image):\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = tf.image.resize(image, [resize_size, resize_size], method=tf.image.ResizeMethod.BILINEAR) #크기 조절\n    image = tf.image.central_crop(image, central_fraction=crop_size / resize_size) #중앙 224x224\n    image = tf.math.divide(image, 255.0) #normalize\n    mean = [0.485, 0.456, 0.406]\n    std = [0.229, 0.224, 0.225]\n    image = (image - mean) / std #다 normalize\n\n    return image\n\n\ndef resize_and_crop(image, resize_size=256, crop_size=224):\n    # Resize with bilinear interpolation\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    resized_image = tf.image.resize(image, [resize_size, resize_size], method=tf.image.ResizeMethod.BILINEAR)\n    cropped_image = tf.image.central_crop(resized_image, central_fraction=crop_size / resize_size)\n    image = tf.math.divide(cropped_image, 255.0) #normalize\n\n    return image\n\ndef ifgsm_attack(model, image, label, epsilon=0.001, num_iter=30, clip_min=0.0, clip_max=1.0):\n    adv_image = tf.identity(image)\n\n    for _ in range(num_iter):\n        with tf.GradientTape() as tape:\n            tape.watch(adv_image)\n            prediction = model(preprocess_for_attack(adv_image))\n            loss = tf.keras.losses.sparse_categorical_crossentropy(label, prediction)\n\n        gradient = tape.gradient(loss, adv_image)\n        perturbation = epsilon * tf.sign(gradient)\n\n        adv_image = tf.clip_by_value(adv_image + perturbation, clip_min, clip_max)\n\n    #adv_image_np = adv_image.numpy().squeeze()  # Squeeze to remove channel dimension\n    del gradient, perturbation\n\n    return adv_image\n\nimport tensorflow as tf\nimport numpy as np\n\ndef ifgsm_attack_transparency(model, masked_image, original_image, label, epsilon=0.001, num_iterations=30):\n    # Ensure the input is a float type\n    masked_image = np.expand_dims(masked_image, axis=0)\n    perturbed_image = tf.cast(masked_image, tf.float32)\n\n    # Loop over the number of iterations\n    for i in range(num_iterations):\n        with tf.GradientTape() as tape:\n            tape.watch(perturbed_image)\n            prediction = model(perturbed_image[..., :3])  # Exclude alpha channel for model prediction\n            loss = tf.keras.losses.sparse_categorical_crossentropy(label, prediction)\n            \n        # Calculate gradients\n        gradient = tape.gradient(loss, perturbed_image)[..., :3]  # Exclude alpha channel in gradient\n\n        # Apply gradients only where the alpha channel is non-zero\n        alpha_channel = perturbed_image[..., -1:]\n        masked_gradient = gradient * tf.cast(alpha_channel > 0, tf.float32)\n\n        # Apply the perturbation\n        perturbed_image = perturbed_image[..., :3] + epsilon * tf.sign(masked_gradient)\n        perturbed_image = tf.concat([perturbed_image, alpha_channel], axis=-1)  # Re-add the alpha channel\n\n        # Ensure the perturbed image is still valid\n        perturbed_image = tf.clip_by_value(perturbed_image, 0, 255)\n        \n    # Replace the areas not attacked with original image\n    # Replace the areas not attacked with the original image\n    mask = alpha_channel > 0\n    mask_float = tf.cast(mask, tf.float32)  # Convert the boolean mask to float\n\n    # Ensure original_image and perturbed_image are of the same type (e.g., float32)\n    original_image_float = tf.cast(original_image, tf.float32)\n    perturbed_image_float = tf.cast(perturbed_image[..., :3], tf.float32)\n\n    # Combine the images using the mask\n    result_image = original_image_float * (1 - mask_float) + perturbed_image_float * mask_float\n    \n    del perturbed_image, mask,original_image_float, perturbed_image_float\n\n    return result_image\n\n\ndef preprocess_for_attack(image):\n\n    # 이미지 정규화\n    mean = [0.485, 0.456, 0.406]\n    std = [0.229, 0.224, 0.225]\n    image = (image - mean) / std\n\n    return image\n\n\ndef plot_images(original, temp, mask, model_name, model_type, file_name, label, model):\n    width_in_inches = height_in_inches = 224 / 100\n    save_paths = [f'{model_name}/{model_type}/original/', f'{model_name}/{model_type}/superpixel/', f'{model_name}/{model_type}/pos_neg/'\n                 , f'{model_name}/{model_type}/ifgsm_super_pixel/', f'{model_name}/{model_type}/ifgsm_full/',\n                 f'{model_name}/{model_type}/ifgsm_negative/', f'{model_name}/{model_type}/ifgsm_pos+neg/',\n                  f'{model_name}/{model_type}/neg/', f'{model_name}/{model_type}/combine/']\n    \n    for save_path in save_paths:\n        if not os.path.exists(save_path):\n            os.makedirs(save_path)\n\n    ##original image###\n    plt.figure(figsize=(width_in_inches, height_in_inches))\n    plt.imshow(original)\n    plt.axis('off')\n    plt.savefig(os.path.join(f'{model_name}/{model_type}/original/', f'{file_name}'), bbox_inches='tight', pad_inches=0)\n    plt.close()\n    \n    \n    ###super pixel + attack super pixel ###\n    masked_positive = np.copy(original)\n    masked_positive = np.concatenate((masked_positive, np.ones((*masked_positive.shape[:-1], 1), dtype=masked_positive.dtype) * 255), axis=-1)  # Add alpha channel\n    masked_positive[mask <= 0, -1] = 0\n    \n    plt.imshow(masked_positive)\n    plt.axis('off')\n    plt.savefig(os.path.join(f'{model_name}/{model_type}/superpixel/', f'{file_name}'), bbox_inches='tight', pad_inches=0)\n    plt.close()\n    \n    masked_positive_adv = ifgsm_attack_transparency(model, masked_positive, original, label)\n    masked_positive_adv = np.squeeze(masked_positive_adv, axis=0)\n\n    plt.imshow(masked_positive_adv)\n    plt.axis('off')\n    plt.savefig(os.path.join(f'{model_name}/{model_type}/ifgsm_super_pixel/', f'{file_name}'), bbox_inches='tight', pad_inches=0)\n    plt.close() \n    \n    \n    ##attack negative pixel###\n    masked_negative = np.copy(original)\n    masked_negative = np.concatenate((masked_negative, np.ones((*masked_negative.shape[:-1], 1), dtype=masked_negative.dtype) * 255), axis=-1)  # Add alpha channel\n    masked_negative[mask >= 0, -1] = 0\n    \n    plt.imshow(masked_negative)\n    plt.axis('off')\n    plt.savefig(os.path.join(f'{model_name}/{model_type}/neg/', f'{file_name}'), bbox_inches='tight', pad_inches=0)\n    plt.close()\n\n    masked_negative_adv = ifgsm_attack_transparency(model, masked_negative, original, label)\n    masked_negative_adv = np.squeeze(masked_negative_adv, axis=0)\n\n    plt.imshow(masked_negative_adv)\n    plt.axis('off')\n    plt.savefig(os.path.join(f'{model_name}/{model_type}/ifgsm_negative/', f'{file_name}'), bbox_inches='tight', pad_inches=0)\n    plt.close()\n\n    \n    #attack pos+negative pixel###\n    masked_combined = np.copy(original)\n    masked_combined = np.concatenate((masked_combined, np.ones((*masked_combined.shape[:-1], 1), dtype=masked_negative.dtype) * 255), axis=-1)  # Add alpha channel\n    masked_combined[mask == 0, -1] = 0\n    \n    plt.imshow(masked_combined)\n    plt.axis('off')\n    plt.savefig(os.path.join(f'{model_name}/{model_type}/combine/', f'{file_name}'), bbox_inches='tight', pad_inches=0)\n    plt.close()\n\n    masked_combined_adv = ifgsm_attack_transparency(model, masked_combined, original, label)\n    masked_combined_adv = np.squeeze(masked_combined_adv, axis=0)\n\n    plt.imshow(masked_combined_adv)\n    plt.axis('off')\n    plt.savefig(os.path.join(f'{model_name}/{model_type}/ifgsm_pos+neg/', f'{file_name}'), bbox_inches='tight', pad_inches=0)\n    plt.close()\n\n    # Lime Mask를 원본 이미지에 적용하여 긍부정 시각화\n    masked_negative = np.zeros_like(original)\n    masked_negative[mask < 0] = [255, 0, 0]\n    \n\n    # Create a new image for positive parts (green color)\n    masked_positive = np.zeros_like(original)\n    masked_positive[mask > 0] = [0, 255, 0]\n\n    # Combine the positive and negative images\n    combined_image = original + masked_negative + masked_positive\n\n    # Display the result with larger size\n    plt.imshow(combined_image)\n    plt.axis('off')\n    plt.savefig(os.path.join(f'{model_name}/{model_type}/pos_neg/', f'{file_name}'), bbox_inches='tight', pad_inches=0)\n    plt.close()\n    \n    original_image = np.expand_dims(original, axis=0)\n    original_attack = ifgsm_attack(model, original_image, label)\n    original_attack = np.squeeze(original_attack, axis=0)\n    plt.imshow(original_attack)\n    plt.axis('off')\n    plt.savefig(os.path.join(f'{model_name}/{model_type}/ifgsm_full/', f'{file_name}'), bbox_inches='tight', pad_inches=0)\n    plt.close()\n    \n    del masked_positive, masked_positive_adv, masked_negative, masked_negative_adv, masked_combined, masked_combined_adv\n    del combined_image, original_image, original_attack\n    \n    plt.cla()\n    plt.clf()\n    \ndef iFGSM_and_LIME(df, i, model):\n    img = cv2.imread(df['path'].iloc[i])\n    selected_image = preprocess_image(img)\n    y = 0 if df['label'].iloc[i]=='dogs' else 1\n    Original_image = resize_and_crop(img)\n    selected_image = np.expand_dims(selected_image, axis=0)\n    prediction = model.predict(selected_image)\n    predicted_class = np.argmax(prediction)\n    explainer = lime.lime_image.LimeImageExplainer(feature_selection='auto')\n    explanation = explainer.explain_instance(selected_image[0], model.predict, top_labels=1, hide_color=0, num_samples=100)\n    temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=10, hide_rest=False)\n    plot_images(Original_image, temp, mask, model_name, df['divide'].iloc[i], \"/\".join(df['path'].iloc[i].split('/')[4:]).replace(\"/\",\"-\"), y, model)\n    del Original_image, selected_image, explainer, explanation, temp, mask, img\n    return ['dogs', 'cats'][predicted_class], df['label'].iloc[i]\n    \ndef file_split(range_, df, model):\n    for i in tqdm(range_):\n        full_prediction, full_true = iFGSM_and_LIME(df, i, model)\n        print(full_prediction, full_true)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T01:44:44.727228Z","iopub.execute_input":"2023-12-13T01:44:44.727600Z","iopub.status.idle":"2023-12-13T01:44:44.774526Z","shell.execute_reply.started":"2023-12-13T01:44:44.727570Z","shell.execute_reply":"2023-12-13T01:44:44.773591Z"}}},{"cell_type":"markdown","source":"if __name__ == '__main__':\n    ctypes.CDLL(\"libc.so.6\").malloc_trim(0) \n    count = 0\n    start = 800\n    resize_size = 256\n    crop_size = 224\n\n    while True:\n        end = 1000\n        df = pd.read_csv('/kaggle/input/dog-cat-pandas/cat_dog_df.csv', index_col=0)\n        df = df[df['divide']=='test']\n        model_name = 'vit'\n        model = keras.models.load_model('/kaggle/input/dog-and-cat-classifier/'+model_name+\"_best_model.h5\", compile=False)\n        model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n        file_split(range(start, end), df, model)\n        gc.collect()\n        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n        start += 500\n        break","metadata":{"execution":{"iopub.status.busy":"2023-12-13T01:44:45.006649Z","iopub.execute_input":"2023-12-13T01:44:45.006934Z","iopub.status.idle":"2023-12-13T01:49:41.328269Z","shell.execute_reply.started":"2023-12-13T01:44:45.006911Z","shell.execute_reply":"2023-12-13T01:49:41.326305Z"}}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}